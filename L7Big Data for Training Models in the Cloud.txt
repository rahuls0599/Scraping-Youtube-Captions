YUFENG GUO: What happens
when our training is
too big to fit on our machine?
Or training in the model
starts to take hours?
We go to the Cloud, of course.
When you have a lot of data
such that you can't reasonably
run the training on
your local machine,
or the size of that data is
larger than your hard drive,
it's time to look
at other options.
One solid option is to shift
the machine learning training
to another computer with
access to more storage,
thus freeing up your
hard drive space,
and allowing you to
work on other things
while that training
is taking place.
Let's break down
exactly what parts need
to be moved into the Cloud.
It's useful to think
about our training
as needing two primary
resources-- compute
and storage.
What's interesting
here is that we
don't have to tie them together
quite as tightly as you
might at first expect.
We can decouple
them, which yields
specialized systems for both.
And this can lead to
efficiencies of scale
when you're dealing
with big data.
Now, compute load is moved
around easily enough.
But the moving of
large data sets,
that can be a bit more involved.
However, if your data
set is truly large,
the results are
worthwhile, as it
allows the data to be
accessed by many machines
in parallel that are
working on your machine
learning training job.
Google Cloud platform
has a couple of easy ways
to tie together
these abstractions.
First, we'll want to make sure
that our data is on Google
Cloud Storage, or GCS.
We can do this using
a variety of tools.
For the smaller to medium
data sets, just use gsutil.
It's a command line tool
that was specifically
made for interacting with
Google Cloud Storage.
And it supports a -m
option that allows
for sending multiple streams
in parallel, thus speeding up
your transfer job.
But if your data is too big
to send over the network,
you can use the Google transfer
appliance, which is literally
a machine that will be
shipped to your data center
to securely capture and transfer
a whole petabyte of data.
With a typical network
bandwidth of, say,
100 megabits per second,
it would take three years
to upload a petabyte of
data over the network.
Even if you had a
gigabyte connection,
it would still take four months.
Now, who wants to
wait that long?
The transfer appliance
on the other hand,
can capture a petabyte
of data in just 25 hours.
That's crazy fast.
Now that our data's
in a Cloud, we're
ready to run machine
learning training at scale.
But that's a whole
topic of its own,
which we will cover
in our next episode.
Training machine-learning
models on large sets
can be challenging to
accomplish with limited compute
and storage resources.
But it doesn't have
to be that way.
By moving to the Cloud via
either gsutil or the transfer
appliance, you can
train on large data sets
without any hiccups.
I'm Yufeng Guo, and thanks for
watching this episode of Cloud
AI Adventures.
On our next episode,
we'll be exploring
the other side of running
machine learning in the Cloud--
the compute side.
We'll even be exploring the
options when it comes to GPUs.
But for now, remember,
when you need
to do machine-learning
on big data sets,
put your data in the cloud.
Thanks for watching
this episode.
And if you're enjoying
this series, please let me
know by liking the video.
And if you want more
machine-learning action,
be sure to subscribe
to the channel
to catch future episodes
as they come out.
Thanks for watching that
episode of Cloud AI Adventures.
You can catch the
whole series here.
And if you have any
thoughts and comments,
feel free to leave
them down below.
